# FewShot_MusicGen
Research project - Performing music generation of target domain with minimal available data from said domain

## Overview 

Project aims to explore the potential of using Generative Adversarial Networks (GANs) and/or diffusion models for few-shot conditional generation of music. Starting with a pre-trained model for piano music generation, we intend to leverage a small dataset of a different musical instrument for fine-tuning and conditioning. The objective is to then generate realistic and coherent music of the other instrument, such as the violin, without explicitly training a model on a large violin dataset. Successful outcomes of this project would mean establishing a method for domain adaptation in the field of music generation.

## Current most relevant resources:

[3] Kong, Z., Ping, W., Huang, J., Zhao, K., & Catanzaro, B. (2021, March 30). DiffWave: A versatile diffusion model for audio synthesis. arXiv.org. Retrieved April 18, 2023, from https://arxiv.org/abs/2009.09761
[10] Goel, K., Gu, A., Donahue, C., & RÃ©, C. (2022, February 20). It's raw! audio generation with state-space models. arXiv.org. Retrieved April 20, 2023, from https://arxiv.org/abs/2202.09729
[11] Zhao, Y., Ding, H., Huang, H., & Cheung, N.-M. (2023, April 15). A closer look at few-shot image generation. arXiv.org. Retrieved April 20, 2023, from https://arxiv.org/abs/2205.03805
